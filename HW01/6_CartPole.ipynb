{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezasakhaei/RL_Course2023_Homeworks/blob/main/HW01/6_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ekcge5vHPpR"
      },
      "source": [
        "# Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ6K6gUyFP8v",
        "outputId": "df06aca5-1732-4e0c-aa24-fef8625a2575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Waiting for headers] [Wai\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Waiting for headers] [Con\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 336 kB in 2s (205 kB/s)\n",
            "Reading package lists... Done\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio==2.4.0 in /usr/local/lib/python3.8/dist-packages (2.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.0) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.0) (1.22.4)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.8/dist-packages (0.27.1)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic_control]) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic_control]) (1.22.4)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic_control]) (0.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic_control]) (4.5.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic_control]) (6.0.0)\n",
            "Requirement already satisfied: pygame==2.1.3.dev8 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic_control]) (2.1.3.dev8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[classic_control]) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!pip install 'imageio==2.4.0'\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip3 install gymnasium[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "lzUJnJrQEN1r"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import base64\n",
        "import random\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import gymnasium as gym\n",
        "from itertools import count\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csY30Op-HVgu"
      },
      "source": [
        "# Utility functions for rendering evironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "B_g_w4-VFqXz"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "    \n",
        "    return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "IH96rj7THfsS"
      },
      "outputs": [],
      "source": [
        "def create_policy_eval_video(env, policy, filename, num_episodes=1, fps=30):\n",
        "  \n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(num_episodes):\n",
        "            state, info = env.reset()\n",
        "            video.append_data(env.render())\n",
        "            while True:\n",
        "                state = torch.from_numpy(state).unsqueeze(0).to(DEVICE)\n",
        "                action = policy(state)\n",
        "                state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "                video.append_data(env.render())\n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "    return embed_mp4(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozFEZ_bqH71W"
      },
      "source": [
        "# Replay Memory and Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "84EJUO1JH7h4"
      },
      "outputs": [],
      "source": [
        "SARS = namedtuple('SARS', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "WD_Av2C7IIZ4"
      },
      "outputs": [],
      "source": [
        "# Complete the Q-Network below. \n",
        "# The Q-Network takes a state as input and the output is a vector so that each element is the q-value for an action.\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        units = [64, 32]\n",
        "        self.lin0 = nn.Linear(n_observations, units[0])\n",
        "        self.lin1 = nn.Linear(units[0], units[1])\n",
        "        self.lin2 = nn.Linear(units[1], n_actions)\n",
        "        self.relu = nn.ReLU()\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        x = self.relu(self.lin0(x))\n",
        "        x = self.relu(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ZW4F6SQeMZ"
      },
      "source": [
        "# Policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6SfSblT_6f"
      },
      "source": [
        "Now we define 2 policies. We use greedy policy for evaluation and e-greedy during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "fokLsyg5Qc41"
      },
      "outputs": [],
      "source": [
        "# This function takes in a state and returns the best action according to your q-network.\n",
        "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network. \n",
        "\n",
        "# state shape: (1, state_size) -> output shape: (1, 1)  \n",
        "def greedy_policy(qnet, state):\n",
        "    state = torch.tensor(state, dtype=qnet.lin0.weight.dtype, device = qnet.lin0.weight.device)\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    with torch.no_grad():\n",
        "        output = qnet(state)\n",
        "    best_action = torch.argmax(output)\n",
        "    best_action = torch.tensor(best_action, dtype=torch.int64, device = qnet.lin0.weight.device)\n",
        "    return best_action\n",
        "    # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "_iE-12xgRc2y"
      },
      "outputs": [],
      "source": [
        "# state shape: (1, state_size) -> output shape: (1, 1)\n",
        "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network.\n",
        "\n",
        "def e_greedy_policy(qnet, state, current_timestep):\n",
        "    \n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * current_timestep / EPS_DECAY)\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    # With probability \"eps_threshold\" choose a random action \n",
        "    # and with probability 1-\"eps_threshold\" choose the best action according to your Q-Network.\n",
        "    \n",
        "    best_action = greedy_policy(qnet, state)\n",
        "    rnd_num = np.random.uniform(0, 1)\n",
        "    if rnd_num > eps_threshold:\n",
        "        return best_action\n",
        "    else:\n",
        "        return torch.tensor(env.action_space.sample(), dtype=torch.int64, device = qnet.lin0.weight.device)\n",
        "    # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PvG1MpOK9mX"
      },
      "source": [
        "# Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "5Sc1a-6ZLAE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "0c0d6bfa-1b2a-40c9-9919-d1d48e49d312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.02057094  0.01676947 -0.01563998 -0.02650603]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <video width=\"640\" height=\"480\" controls>\n",
              "    <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAACKZtZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1NSByMjkxNyAwYTg0ZDk4IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxOCAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yNS4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAABomWIhAA3//72rvzLK0cLlS4dWXuzUfLoSXL9iDB9aAAAAwAAAwAAJuKiZ0WFMeJsgAAALmAIWElDyDzETFWKgSvGXwOeIADoU/kDp1Un3QPFpQecGAzVZhYuxl5z4zoaXmU3f5iaji5AmO4Y3mCSXZY3JkQb8lL3n3UZCzkuYQdVx3F62emLhrR58JdRk+TJcWA/2OP93x8B0S0elTaZk+8f89EQNE1vACU4H4KATIm12czW3/yKe8Ur1GQsD0fM8kG1LVf39hYZwoaMTqsX8exxOSXd1U7H7/o8/A3juxvTGiXOiojxcmr2gVwg+P7dhnRM1Q1yACwATrrHdOkUuUH0YnIh/DcSuZB984d3qzyplXlcjUXQsV4XvZhqwx2GadN/H3rYCxRKPxQUrxdkwrlXZW6YaFaGJ4uEK8Nn8Bv1a7ugy9XiHKrlu28c9dJdIJ8EHt0ksdNOFf20UIAAAjeSoPg0AYZqVjWSNrppcDdHaZT8jYDN80I9I+Rbsw5GrOhQANgRIuv/YirLfUle4whvKqYC6JV4SWAAAAMAAAMAJGEAAACXQZokbEM//jhAAAENHyQvyNS8M+B/3VdLURC9+y+Y5V4bW4a0xYpjeBCNpUMrm4+xK0pf8+k6XvN2aUfGBPKFdz8wh/3qGOc45mdqAlNO9U7WKW5AhiaH9uVPYVgyD0Mlxqpven8rqYz7vWHgjrJegEmHqiCIXh03ncLOd+pPTeCcp2RNhslTSbB0WpO0nMZNcfpvQd7GYAAAACVBnkJ4hX8AABa+S6+stYRnKZ3gsdg6KhEMADTfpeEJCMpGK3EfAAAAFgGeYXRCfwAADS4VwogDnbD0s3GSpoAAAAAvAZ5jakJ/AAAjscwTtyPZXc6FXuLACdYqQcVYgiBsINrSbransQcdjdhu8FmZaVEAAAEGQZpnSahBaJlMCF///fEAAAMCo8n12BvrwsALXTa44kurXUPwh3i7nGB6pIiS0TbM6xEJDuGew3Z+3VUtSqBSvwuw2x+o5CV1LP+hRpvc5jpXn1XWp5I9aHoG1FMZkTG8JLMukntvn8fOm1yEbN4GNUdRAVIjxGyE+kgdP/UlQDiYLXOFvpIBvE3Ez9EpcE3iwfP/HREdYoCU601pITI4hwv1XTArEAG5JWfFuSkgiUFj97lUpwTgpbD6vDfghH4gnkAJHZny+xfpAxdehP3FySxK/aZ3O2Xjp3rz4AbTU6o6JC22w0JZ7FmFcsJCc7Uq8HoGT7pGztbBnjPeJPVu7UEW7LV8iwAAAEJBnoVFESwr/wAAFruUVLFR7CN7i4WNvREv6vNTFi3gqTcw6l5KKZ7/UlIXPIPm0lOdK235pHuQGXUIRYjQ+G0WYi0AAABHAZ6makJ/AAAjvXPCBtKhhSTMBcB+DJ7z2Nb5YS/Lbh80jFWHrsqSSndoKmUMGyagVQX/KJkBlIIa68VEuU7lWqFU9TBHq4sAAADCQZqqSahBbJlMCE///IQAAA+IaM3suoAEI7SG0HaHkcxw1/W/68p2+E6Q0S5NMDqCrRT7XY/HkoAf1/jdBqsA7aahzZBDLzX3ClOXokLoJ/njgOvDIHTLAoW3qWZ+68AswNIdm93zthI4ufYYGQI4xYoWY6QrtfqR3rwNkDf8lWW6GuzHpxbU3lwkFrkYZsjKJMfR8wP1AbwpZhWWZzjuvvIIqqZppQd9AWbw9/l+kqniU0fDmGq8TjyJ9zfQFWwJPp4AAABqQZ7IRRUsK/8AABYSndlBeTKP2zcJq4lAZ4BG0FA3+itmoWYIwgYjLB6LQNMMhzBoJ+8QA1K2gZdTI2pxYS//qF6t7ADUapXLdYUUvz7bt//7BK1A6K0JCk3+1iaAw39jBTB9zzEXTyCPgAAAAGIBnulqQn8AACKSE9PfP4T+lQmIO+nYsKgglRgsbYiFQNlCklvIuiGUtSkWh8bo5ebp+13rU8X37Sk1KeUcHqa00oRpL+tLRnjjzcT1DGbR13+DfRulSxAERqfqL7/aPhzpUQAAA4Vtb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAABbwABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAACr3RyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAABbwAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAACYAAAAZAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAAW8AAAQAAAEAAAAAAidtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADwAAAAWAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAHSbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAABknN0YmwAAACWc3RzZAAAAAAAAAABAAAAhmF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAACYAGQAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAwYXZjQwFkAB7/4QAYZ2QAHqzZQJgzoQAAAwABAAADADwPFi2WAQAFaOvnLIsAAAAYc3R0cwAAAAAAAAABAAAACwAAAgAAAAAUc3RzcwAAAAAAAAABAAAAAQAAAFhjdHRzAAAAAAAAAAkAAAABAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAgAAAAAAgAAAgAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAAsAAAABAAAAQHN0c3oAAAAAAAAAAAAAAAsAAARYAAAAmwAAACkAAAAaAAAAMwAAAQoAAABGAAAASwAAAMYAAABuAAAAZgAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1OC4yOS4xMDA=\" type=\"video/mp4\">\n",
              "    Your browser does not support the video tag.\n",
              "    </video>"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "MIN_MEMORY = 500\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "n_actions = env.action_space.n\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "q_network = DQN(n_observations, n_actions).to(device)\n",
        "target_network = DQN(n_observations, n_actions).to(device)\n",
        "\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"random_agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWq08ZENXx6h"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USGbCrKbFusn"
      },
      "outputs": [],
      "source": [
        "num_episodes = 200\n",
        "episode_returns = []\n",
        "episode_durations = []\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    # 1. Start a new episode\n",
        "    episode_returns.append(0)\n",
        "    episode_durations.append(0)\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    i = 0\n",
        "    while not done:\n",
        "        # 2. Run the environment for 1 step using e-greedy policy\n",
        "        action = e_greedy_policy(q_network, state, i_episode)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(int(action.cpu().numpy()))\n",
        "        done = terminated or truncated\n",
        "        # 3. Add the (state, action, next_state, reward) to replay memory\n",
        "        # matching everything\n",
        "        train_dev = q_network.lin0.weight.device\n",
        "        train_dtype = q_network.lin0.weight.dtype\n",
        "        state = torch.tensor(state, device = train_dev, dtype = train_dtype)\n",
        "        reward = torch.tensor(reward, device = train_dev, dtype = train_dtype)\n",
        "        next_state = torch.tensor(next_state, device = train_dev, dtype = train_dtype)\n",
        "        action = torch.tensor(action, device = train_dev, dtype = train_dtype)\n",
        "        # pushing to the buffer\n",
        "        memory.push(SARS(state, action, next_state, reward))\n",
        "        if len(memory) > MIN_MEMORY:\n",
        "            # 4. Optimize your q_network for 1 iteration\n",
        "            #       4.1 Sample one batch from replay memory\n",
        "            sample = memory.sample(BATCH_SIZE)\n",
        "            batch = SARS(*zip(*sample))\n",
        "            #       4.2 Compute predicted state-action values using q_network\n",
        "\n",
        "            #       4.3 Compute expected state-action values using target_network (Don't forget \"no_grad\" because we don't want gradient through target_network)\n",
        "            #       4.4 Compute loss function and optimize q_network for 1 step\n",
        "\n",
        "            # 5. Soft update the weights of target_network\n",
        "            #       θ′ ← τ θ + (1 −τ )θ′\n",
        "            #       θ   is q_network weights\n",
        "            #       θ′  is target_network weights   \n",
        "            theta = q_network.state_dict()\n",
        "            theta_prime = target_network.state_dict()\n",
        "            for key in theta_prime:\n",
        "                theta_prime[key] = TAU*theta[key] + (1-TAU)*theta_prime[key]\n",
        "            target_network.load_state_dict(theta_prime)\n",
        "\n",
        "        # 6. Keep track of the total reward for each episode to plot later\n",
        "        episode_returns[i_episode] += (GAMMA ** (i)) * reward\n",
        "        i += 1\n",
        "\n",
        "    episode_durations[i_episode] = i\n",
        "    # ==================================== Your Code (End) ====================================  \n",
        "\n",
        "print('Complete')\n",
        "plt.plot(range(1, num_episodes+1), episode_durations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s50clnmF_az"
      },
      "outputs": [],
      "source": [
        "# Render trained model\n",
        "\n",
        "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"trained_agent\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGUSb9m3qvX-",
        "outputId": "d0ba0251-8220-4ab0-b238-bb18b8f47069"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 'b', 'c', 'd'), (1, 2, 3, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "*[('a', 1), ('b', 2), ('c', 3), ('d', 4)]"
      ],
      "metadata": {
        "id": "iqgab4ZXq0Bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}